# -*- coding: utf-8 -*-
"""Home_Depot_Recalls

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SGg1nXE_8UVtIsGYlw0Acf1LA9hmSnt2
"""

import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

data = pd.read_csv("/content/IncidentReports_cleaned.csv", encoding='latin1', sep=',', skiprows=[0], on_bad_lines='skip', engine='python')
data.columns

#I only want a sample size of 30,000
data = data.sample(n=30,000 random_state=42)

data.isna().sum()
#Drop everything till Manu, then drop brand / model name / serial number / upc / date created

"""Data Cleaning"""

data = data.drop(columns=["Category of Submitter","Publication Date","Report No .","Product Code",'Answer Explanation','My Relation To The (Primary) Victim','If Not Do You Plan To','Have You Contacted The Manufacturer','Damage Repaired','Damage Description','Product Was Damaged Before Incident','Submitter Has Product','Retailer','Date Manufactured','Sent to Manufacturer / Importer / Private Labeler', 'ZIP', 'City', 'Incident Description', 'Purchase Date Is Estimate', 'Purchase Date', 'Retailer State', 'Manufacturer Date Code', 'Associated Report Numbers', 'Product Was Modified Before Incident', 'Brand', 'Model Name or Number', 'Serial Number', 'UPC', 'Company Comments'], errors='ignore')

data.isna().sum()

#fill age with mean
data["(Primary) Victim's Age (years)"].fillna(data["(Primary) Victim's Age (years)"].mean(), inplace=True)

#drop that line
data = data.dropna(subset=['Product Type'])

data.head()

#Identify Target unique values and their count
data["(Primary) Victim Severity"].value_counts()

#combine unknown and unspecified
data["(Primary) Victim's Sex"] = data["(Primary) Victim's Sex"].replace("Unknown", "Unspecified")
data["(Primary) Victim's Sex"].value_counts()

data.head()

#create a csv of the data at the point
data.to_csv('/content/IncidentReports_cleaned.csv', index=False)
print("DataFrame saved to /content/IncidentReports_cleaned.csv")

"""**Processing data for modeling**"""

X=data.drop(columns=["(Primary) Victim Severity"])
y=data["(Primary) Victim Severity"]

num_cols=["(Primary) Victim's Age (years)"]
cat_cols=["Product Type","(Primary) Victim's Sex"]

X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=.2,random_state=42)

numeric_transformer=MinMaxScaler()
categorical_transformer=OneHotEncoder(handle_unknown="ignore",sparse_output=False)

preprocessor=ColumnTransformer(
    transformers=[
        ("num",numeric_transformer,num_cols),
        ("cat",categorical_transformer,cat_cols)
    ]
)
preprocessor

z = preprocessor.fit_transform(X_train)
col_names = preprocessor.get_feature_names_out()
df_train = pd.DataFrame(z, columns=col_names, index=X_train.index)
df_train.head()

model_DT=Pipeline(steps=[
    ("preprocess",preprocessor),
    ("clf",DecisionTreeClassifier(max_depth=4,random_state=42))
])
model_DT

model_DT.fit(X_train,y_train)

y_pred = model_DT.predict(X_test)
print(y_test)
y_pred

y_pred = model_DT.predict(X_test)

print("Accuracy:", accuracy_score(y_test, y_pred))
print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred, digits=3))

model_RF=Pipeline(steps=[
    ("preprocess",preprocessor),
    ("rf",RandomForestClassifier(n_estimators=3,random_state=42,max_depth=10))
])
model_RF

model_RF.fit(X_train,y_train) #Random Forest Model

#Random Forest Model
y_pred = model_RF.predict(X_test)

print("Accuracy:", accuracy_score(y_test, y_pred))
print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred, digits=3))

"""### Handling Class Imbalance

Given the severe class imbalance in the target variable, we will use resampling techniques to create a more balanced dataset for training. We'll use the `imblearn` library for this.

First, let's install `imblearn`.
"""

!pip install imblearn

"""Now, we will apply resampling. We'll use `SMOTE` for oversampling the minority classes and `RandomUnderSampler` for undersampling the majority class. It's crucial to apply resampling *after* splitting the data into training and testing sets, and only to the *training set* to prevent data leakage."""

from imblearn.over_sampling import SMOTE
from imblearn.under_sampling import RandomUnderSampler
from imblearn.pipeline import Pipeline
from sklearn.preprocessing import MinMaxScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer

# Define the preprocessing steps for numerical and categorical features
numeric_transformer=MinMaxScaler()
categorical_transformer=OneHotEncoder(handle_unknown="ignore",sparse_output=False)

preprocessor=ColumnTransformer(
    transformers=[
        ("num",numeric_transformer,num_cols),
        ("cat",categorical_transformer,cat_cols)
    ]
)

# Define the combined pipeline with preprocessing and resampling
# Resampling is applied AFTER preprocessing for categorical features
# and BEFORE the final classifier.

# For multi-class classification, SMOTE typically works class by class
# and RandomUnderSampler can be used to balance after oversampling.
# We can specify `sampling_strategy` for more control if needed.

# Note: SMOTE needs numerical data, so it should come after OneHotEncoder
# Let's create a pipeline that first preprocesses, then resamples.

# Create a pipeline for preprocessing and resampling
# The preprocessor is already set up to handle numerical and categorical features.
# After preprocessing, we'll apply resampling.
# We will use a smaller k_neighbors for SMOTE to avoid issues with very small classes.

# Define a pipeline for the training data to apply preprocessor and then resampling
pipeline_resample = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('smote', SMOTE(random_state=42, k_neighbors=1)), # k_neighbors=1 for very small classes
    ('undersample', RandomUnderSampler(random_state=42))
])

# Fit and transform the training data
X_train_resampled, y_train_resampled = pipeline_resample.fit_resample(X_train, y_train)

print("Original training target distribution:\n", y_train.value_counts())
print("\nResampled training target distribution:\n", y_train_resampled.value_counts())

"""With the resampled training data, you can now train your models (Decision Tree and Random Forest) to see if the performance, especially for the minority classes, improves. Remember to use `X_train_resampled` and `y_train_resampled` for training."""

model_RF_resampled = RandomForestClassifier(n_estimators=200, random_state=42, max_depth=30, class_weight='balanced')
model_RF_resampled.fit(X_train_resampled, y_train_resampled)

# Preprocess X_test using the already fitted preprocessor
X_test_preprocessed = preprocessor.transform(X_test)

# Make predictions with the resampled Random Forest model
y_pred_resampled = model_RF_resampled.predict(X_test_preprocessed)

print("Accuracy (Resampled Random Forest):", accuracy_score(y_test, y_pred_resampled))
print("\nConfusion Matrix (Resampled Random Forest):\n", confusion_matrix(y_test, y_pred_resampled))
print("\nClassification Report (Resampled Random Forest):\n", classification_report(y_test, y_pred_resampled, digits=3))

# Assuming your X_train and y_train are still available
from imblearn.combine import SMOTETomek
from imblearn.pipeline import Pipeline as ImbPipeline
from sklearn.compose import ColumnTransformer

# ... (preprocessor definition remains the same as in cell [26]) ...

# New pipeline using SMOTETomek
pipeline_smotetomek = ImbPipeline(steps=[
    ('preprocessor', preprocessor),
    ('smotetomek', SMOTETomek(random_state=42)) # Using SMOTETomek for balanced over/undersampling
])

# Fit and transform the training data
X_train_new_resampled, y_train_new_resampled = pipeline_smotetomek.fit_resample(X_train, y_train)

print("SMOTETomek training target distribution:\n", y_train_new_resampled.value_counts())

# Train the optimized Random Forest model on the new resampled data
model_RF_smotetomek = RandomForestClassifier(
    n_estimators=200,
    random_state=42,
    max_depth=30,
    class_weight='balanced'
)
model_RF_smotetomek.fit(X_train_new_resampled, y_train_new_resampled)

# Evaluate the model
X_test_preprocessed = preprocessor.transform(X_test)
y_pred_smotetomek = model_RF_smotetomek.predict(X_test_preprocessed)

print("\n--- SMOTETomek Random Forest Evaluation ---")
print("Accuracy:", accuracy_score(y_test, y_pred_smotetomek))
print("\nClassification Report:\n", classification_report(y_test, y_pred_smotetomek, digits=3))

data.to_csv('/content/IncidentReports_processed.csv', index=False)
print("DataFrame saved to /content/IncidentReports_processed.csv")